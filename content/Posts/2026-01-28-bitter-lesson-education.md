---
title: A bitter lesson for higher education
type: post
aliases:
  - The Bitter Lesson for Education
description: Rich Sutton's "Bitter Lesson" showed that general computation beats human-crafted expertise in AI. Higher education faces a parallel lesson - our complex assessment frameworks, built on the difficulty of artifact generation, are collapsing now that AI makes production trivial.
author: "[[Michael Rowe]]"
date: 2026-01-28
updated: 2026-01-28
tags:
  - artificial-intelligence
  - higher-education
  - assessment
  - learning
category: Assessment
related:
  - "[[learning-alignment]]"
draft: false
permalink: ""
enableToc: true
cssclasses:
  - ""
---
In 2019, AI researcher Rich Sutton published ["The Bitter Lesson"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), reflecting on 70 years of artificial intelligence research. His observation was striking: researchers had repeatedly invested in encoding human knowledge into AI systems—chess strategies, speech recognition rules, computer vision features—only to find that simpler computational approaches using search and learning eventually outperformed their carefully crafted expertise. The "bitter" part wasn't just that computation won; it was that decades of intuitive, satisfying work had been directed toward approaches that couldn't scale (other have made similar arguments related to [self-driving cars](https://stratechery.com/2024/elon-dreams-and-bitter-lessons/)). Higher education now faces a parallel lesson about how we measure learning.

## The difficulty of generation as validity

We've built our assessment systems around a particular kind of scarcity: the cognitive difficulty of producing artifacts. A 3000-word essay with proper structure, academic voice, and appropriate citations required sustained engagement with material, and this difficulty seemed to validate the essay as a learning measure. The effort appeared to correlate with depth of understanding. The visible output provided something concrete we could evaluate.

We've encoded our intuitions about "what good work looks like" into elaborate frameworks. Detailed rubrics specify grade boundaries: a first-class essay demonstrates "critical synthesis of diverse sources" while a 2:1 shows "clear understanding with some synthesis." Marking schemes categorise assignments into weighted components. Academic integrity policies assume individual artifact production indicated individual learning. These frameworks have become codified across institutions, embedded in quality assurance processes and professional standards. And we stopped questioning whether they measured what we claimed to measure.

The progression seemed logical. To ensure fairness and reduce ambiguity, we created highly prescribed assignments with explicit success criteria. "Section 1 should address the theoretical framework (500 words), Section 2 should apply this to your chosen case study (1000 words), Section 3 should critically evaluate limitations (500 words)." We shared detailed marking rubrics with students. We provided scaffolding to support those who might struggle with open-ended tasks. We standardised conditions to create comparable outputs. This equity-focused design—removing barriers, providing clarity, ensuring fairness—represented assessment best practice.

## What we inadvertently optimised

Then large language models made artifact generation computationally trivial, revealing something uncomfortable about our processes: in stripping away ambiguity to ensure fairness, we've written perfect prompts for AI. Our detailed instructions provide exactly the context LLMs need to complete tasks. The clearer our success criteria for students, the better AI performs when asked to. The more we scaffold, the more we optimise for computational reproduction. We have, with the best intentions, designed assessment tasks that remove precisely the elements—personal interpretation, contextual judgement, genuine synthesis—where human capabilities remain distinctive.

This creates what [Dawson et al. (2024)](https://www.tandfonline.com/doi/full/10.1080/02602938.2024.2386662) describe as a psychometric problem, not merely an integrity crisis. When measured behaviour can be outsourced, the measurement loses interpretive accuracy. The score no longer tells us what we think it tells us. I think this is a useful distinction because it moves the conversation about from being about student morality and makes it about measurement validity. If we can't trust the outcome of the assessment we can't make inferences about the competence of the student.

Consider [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law): when a measure becomes a target, it ceases to be a good measure. We originally intended to measure learning through artifacts. But once artifacts became the target, students rationally optimised for artifact production. Essay-writing skills became distinguishable from disciplinary understanding. Students learned to navigate rubrics, match assessor expectations, and produce academically acceptable work; capabilities you can develop independently of deep learning. When artifact generation was difficult, this optimisation was masked by the effort required. The correlation between production capability and learning appears stronger than it is.

We should acknowledge that the difficulty of artifact generation never actually validated the measure. Even before AI, essays didn't guarantee learning had occurred. Students could produce acceptable artifacts through surface learning (or through essay mills, or copy/paste and editing). The form of the activity can be mastered without understanding it. But the cognitive load required to generate these artifacts seemed to create validity because we couldn't easily separate artifact production from learning. The barrier to entry filtered for those with relevant capabilities, and outsourcing was expensive and risky enough to maintain the system's perceived integrity.

## Why this can't be fixed

What I'm seeing now across the sector is an attempt to make artifacts more difficult to generate; handwritten exams, proctored environments, or novel tasks that AI handles poorly. But I think this addresses the wrong problem. The issue isn't that AI can now do what students used to do; it's that what students used to do wasn't measuring learning as directly as we assumed. Adding authentication doesn't restore validity when the fundamental premise—that artifact quality indicates learning—has been exposed as unreliable.

This parallels Sutton's observation about AI research. Researchers didn't just need better methods for encoding human knowledge; they needed to accept that encoding human knowledge was the wrong strategy. Similarly, we can't retrofit artifact-based assessment by making artifacts harder to produce. The entire structure assumes that artifact scarcity provides evidential value. When that scarcity disappears, modifications to the task difficulty don't address the underlying measurement problem.

The frameworks we've developed over decades—external examining based on artifact quality, programme validation reviewing assessment specifications, standardisation exercises ensuring marker reliability—all rest on the assumption that artifact difficulty correlates with learning validity. These aren't wrong implementations of a sound principle; they're sophisticated implementations of a flawed premise.

This is particularly challenging because the system rewarded specific capabilities that correlate with social and cultural capital: familiarity with academic discourse, exposure to formal writing conventions, understanding of institutional expectations. By measuring artifact production rather than learning, we created stratification based on navigation skills. As [Rose Luckin notes (2025)](https://www.linkedin.com/posts/rose-luckin-5245003_educationreform-humanintelligence-aiineducation-activity-7376880730565672960-ghsr/), we designed systems that reward precisely the capabilities that large language models excel at—pattern matching in text, structured argument construction, formal academic voice, surface coherence without deep understanding. We've inadvertently optimised for machine capabilities.

## Accepting what this means

The bitter lesson for AI research wasn't that researchers failed to implement good ideas; it was that their fundamental approach wouldn't scale regardless of implementation quality. The bitter lesson in higher education is similar: we can't salvage artifact-based assessment through better rubrics or stricter authentication. The foundation itself—measuring learning through artifact difficulty—was contingent on technological constraints that no longer apply.

This doesn't mean our previous work was worthless or that we were foolish. The assessment systems we built reflected genuine efforts to ensure fairness, maintain standards, and support student learning. But we mistook a contingent barrier (difficulty of generation) for a fundamental principle (validity of measurement). When the barrier disappeared, the principle collapsed.

Now we face a choice. We can continue spending energy trying to restore artificial scarcity by making artifact production difficult enough that the old system still appears to function. Or we can accept that computational abundance has revealed what was always true: artifacts were proxies for learning, and proxies are most convincing when they're difficult to fake. The question isn't how we make artifacts harder to generate. It's what learning actually looks like when we're not confusing it with artifact production.

This acceptance is uncomfortable because it requires acknowledging that the frameworks we've refined over decades addressed the wrong question. They asked "how do we evaluate artifacts?" when we should have asked "how do we recognise learning?" These are fundamentally different questions. The former is now computationally trivial to game; the latter remains genuinely difficult (which, for me, suggests it might be the right question to pursue).