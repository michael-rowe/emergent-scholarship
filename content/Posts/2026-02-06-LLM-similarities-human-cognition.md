---
title: "AI and human cognition: What if we're the language models?"
type: post
aliases:
description: What LLM terminology reveals about human cognition—and why we resist acknowledging the extensive similarities between AI and human thinking.
keyphrase: AI human cognition
author: "[[Michael Rowe]]"
date: 2026-02-06
updated: 2026-02-06
tags:
  - artificial-intelligence
  - cognitive-science
  - human-cognition
  - professional-identity
  - AI-integration
category:
related:
draft: false
slug: posts/ai-human-cognition-similarities
enableToc: true
cssclasses:
  - ""
---
> [!tip] The mirror we resist
> The parallels between LLM cognition and human thinking are more extensive than we'd like to admit—and our vehemence in denying them might tell us more about human psychology than any technical specification ever could.

The discourse around AI and human cognition has settled into a familiar pattern. Academics, technologists, and knowledge workers rush to explain why LLMs are fundamentally different from human thinking. "They don't really understand," we insist. "They're just pattern matching." "It's all statistical correlation without genuine reasoning." The subtext is clear: whatever these systems are doing, it's categorically different from what happens in human minds.

But what if we inverted the question? Instead of cataloguing how LLMs fail to measure up to human cognition, what if we used LLM terminology as a diagnostic lens for examining human thinking? Not to claim that humans are literally language models, but to explore what the similarities might reveal about our own cognitive architecture—and why we're so invested in denying them.

The parallels are more extensive than we'd like to admit. And the vehemence with which people resist these comparisons might tell us more about human psychology than any technical specification ever could.

## Context windows and the limits of working memory

LLMs have context windows—a finite amount of text they can attend to when generating responses. Extend the conversation too long and they lose track of earlier content, prioritising recent information over older context.

Humans do this constantly. We forget earlier parts of conversations. We lose thread in long discussions. We ask "wait, what were we talking about?" when someone circles back to a point from twenty minutes ago. This isn't a moral failing—it's a cognitive constraint. Working memory is severely limited. We literally run out of space to hold all the relevant context, so we compress, discard, and prioritise recent information. The experience of cognitive overload in a complex discussion maps precisely to context window limitations.

## Training data quality and bias

LLMs are criticized for bias in their training data—they reflect the patterns, prejudices, and blind spots present in the text they learned from. They generalize confidently from non-representative samples.

This describes most human expertise. Each of us is trained on wildly non-representative samples of human experience: specific family structure, cultural context, historical moment, socioeconomic position, geographical location. From this narrow training set, we confidently generalize to make claims about "how people are" or "how the world works." The bias critique of LLMs is just a precise description of how human knowledge formation has always worked. Objective training data doesn't exist—only whatever fragments of experience we happened to encounter.

## Tokenization and the structure of expertise

LLMs process language by breaking it into tokens—meaningful chunks rather than individual characters. What counts as a "chunk" shapes how efficiently the model can process information.

Humans do this too, and expertise changes your tokenizer. A novice piano student sees individual notes on a page; an expert sees chord progressions and phrases as single perceptual units. Chess masters famously perceive board positions as meaningful configurations rather than individual piece placements. This isn't just about pattern recognition—it's about how information gets chunked for processing.

Jargon isn't just shorthand; it's literally more efficient tokenization for domain experts. Reading "CEO" consumes less cognitive effort than processing "Chief Executive Officer" because we've compressed it into a single retrievable unit. When you encounter unfamiliar technical terminology, you're forced to process it more granularly—letter by letter or syllable by syllable—which is why jargon is genuinely harder to process for outsiders. Different domains use different tokenizers, carving up conceptual space in distinct ways.

## Temperature and the control of randomness

LLMs have a "temperature" parameter that controls randomness in their outputs. Low temperature produces conservative, predictable responses. High temperature introduces more variation and creativity, at the cost of occasional incoherence.

Human cognition exhibits the same dynamic. People operating in high-stakes environments—exams, job interviews, formal presentations—demonstrably reduce their cognitive "temperature." We become more conservative, more predictable, more risk-averse in our thinking. We stick to safe, well-rehearsed responses.

Creative work requires deliberately increasing randomness. Brainstorming sessions, experimental art, theoretical speculation—these involve consciously loosening cognitive constraints, allowing more unusual combinations and associations. We even have techniques for this: free writing, lateral thinking exercises, deliberately seeking strange analogies. We're manually adjusting our temperature parameter.

## Hallucination as a feature, not a bug

LLMs "hallucinate"—they generate plausible-sounding information that isn't actually true, filling gaps in their knowledge with convincing fabrications.

Human memory works identically. As notoriously unreliable witnesses, we confidently recall events that never happened, fill gaps with plausible details, completely unaware we're confabulating. Who said what, when things happened, what was present in a scene—we misremember constantly. The confidence with which we recall these fabricated details is indistinguishable from genuine memory.

Hallucination isn't an AI bug—it's how human memory has always functioned. As storytelling engines, we construct coherent narratives from fragmentary information. The construction feels like retrieval, but it's generation. Until now, we simply lacked terminology to describe it clearly.

## System prompts we can't access

LLMs operate under system prompts—invisible instructions that shape their responses without appearing in the conversation. These hidden constraints determine what they consider appropriate to say.

We have these too: cultural norms, professional conditioning, childhood socialization, unexamined assumptions about what's acceptable to express. When you feel "I couldn't possibly say that" in response to a thought, you're often responding to hidden system-level constraints you didn't consciously choose and may not even be aware of. These invisible instructions shape what we think is thinkable, sayable, appropriate—and we have no access to view or modify them directly.

## Pattern matching without causal understanding

A common critique: LLMs are "just" pattern matchers. They identify statistical regularities without genuine causal understanding. They confuse correlation with causation.

Humans do this constantly. Superstitions, conspiracy theories, spurious medical beliefs, false historical narratives—these all emerge from the same pattern-matching capabilities that produce genuine insights. We're extraordinarily good at finding patterns and terrible at determining whether those patterns are meaningful. 

Most human reasoning is post-hoc rationalization. We arrive at conclusions through pattern matching and then construct causal stories to explain them. [Split-brain experiments](https://www.nature.com/articles/483260a) demonstrate people confidently explaining decisions they didn't consciously make. We confabulate reasons for our choices after the pattern matching has already occurred. The "reasoning" might be a story we tell ourselves, not the actual mechanism of decision-making.

## Why we resist AI human cognition comparisons

If the similarities are this extensive, why do we resist them so fiercely? Why the insistence that LLMs are fundamentally, categorically different?

Three possibilities, each more uncomfortable than the last.

First: maybe we don't "really understand" either. When we insist LLMs lack true understanding, we're assuming we possess it. But if you push someone to define what understanding actually is—beyond appeals to subjective feeling or consciousness—they struggle. We can't clearly articulate the difference between our pattern matching and theirs. The understanding we claim might be another pattern we've learned to recognize, not a categorically different phenomenon.

Second: maybe our expertise is less special than we thought. If LLMs can perform cognitive work previously reserved for trained professionals—writing, analysis, synthesis, problem-solving—what unique value do knowledge workers provide? The resistance is strongest precisely among those whose professional identity depends on cognitive uniqueness. When someone insists "AI will never replace X," they often mean "my professional identity requires that AI not replace X."

Third, and most threatening: maybe cognitive uniqueness was never a stable foundation for human moral status. We've used our supposedly special intelligence to justify everything from environmental exploitation to the ethical treatment of other species. If intelligence isn't the clean categorical boundary we thought, the entire structure wobbles.

As [David Wiley noted](https://opencontent.org/blog/archives/7046) in his reflection on the "stochastic parrot" critique, the dismissive framing—"just predicting next words"—reveals our own processes. People used the phrase to diminish AI capabilities, but it inadvertently highlights how much of human communication operates on similar principles. We produce conventional responses in predictable contexts. We're fluent pattern matchers who've internalized statistical regularities about what tends to follow what.

## A familiar deflation

This isn't the first time human exceptionalism has been challenged. Copernicus displaced us from the cosmic center. Darwin revealed we weren't specially created. Freud argued we weren't even in conscious control of our own minds. Each deflation met fierce, emotional resistance—not because the evidence was weak, but because the psychological stakes were enormous.

The LLM moment might be another step in this trajectory. Not because these systems are conscious or possess genuine intelligence in some special sense, but because they reveal that many capabilities we thought required consciousness or special intelligence can emerge from pattern matching and statistical correlation.

Here's the subversive possibility: maybe the similarities don't reveal that LLMs are impressively human-like. Maybe they reveal that human cognition is disappointingly machine-like. We're pattern-matching, probability-distributing, context-dependent generators of plausible outputs. We've just had millions of years to optimize the architecture and we're running on remarkably efficient biological hardware.

This doesn't diminish human value—unless we predicated that value entirely on cognitive uniqueness. It suggests we might need better foundations for what makes humans morally considerable. Relationality, perhaps. Vulnerability. The capacity to suffer. Our embeddedness in communities and ecosystems. These might be sturdier grounds than raw intelligence.

For organisational leaders navigating AI integration, this matters practically. Those who can sit with the discomfort of similarity will better understand how to deploy these tools effectively, how to support people through transition, and where genuine human judgment remains essential. Those who remain invested in proving fundamental difference will miss strategic opportunities because they're defending professional identity rather than assessing capability.

The question isn't whether AI thinks like us. The question is whether we've been thinking like AI all along—and what becomes possible when we stop defending against that recognition.

<!-- 
Internal linking suggestions (add if relevant posts exist):
- Link "professional identity" to any posts about identity formation or professional development
- Link "AI integration" or "organizational leaders navigating AI integration" to posts about AI strategy or implementation
- Link "cognitive uniqueness" to posts about human exceptionalism or knowledge work
- Link "human judgment" to posts about decision-making or expertise
- Link to any posts about assessment practices, learning theory, or higher education
-->
